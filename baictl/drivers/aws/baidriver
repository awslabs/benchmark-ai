#!/bin/bash

print_unsupported_verb() {
    local object=$1
    local verb=$2
    printf "Unsupported verb ${verb} for object ${object}\n"
}

_create_configmap_yaml_from_terraform_outputs() {
    local bash_src=$(realpath ${BASH_SOURCE})
    local src_dir=$(dirname "${bash_src}")
    (
        export KUBECONFIG=${kubeconfig}
        python3 ${src_dir}/add_terraform_outputs_as_configmap.py outputs-infrastructure || return 1
    )
}

create_infra() {
    local cluster_name=""
    local prefix_list_id=""
    local ssh_access_cidr_blocks=""
    local validate=true

    for arg in "$@"; do
        case "${arg}" in
        --name=*)
            cluster_name="${arg#*=}"
            ;;
        --aws-region=*)
            region="${arg#*=}"
            ;;
        --aws-prefix-list-id=*)
            prefix_list_id="${arg#*=}"
            ;;
        --aws-ssh-access-cidr-blocks=*)
            ssh_access_cidr_blocks="${arg#*=}"
            ;;
        --extra-users=*)
            extra_users="${arg#*=}"
            ;;
        --extra-roles=*)
            extra_roles="${arg#*=}"
            ;;
        --no-validate)
            validate=false
            ;;
        esac
    done

    #Temporary behavior
    [ -z "$prefix_list_id" ] && printf "Missing required argument --aws-prefix-list-id\n" && return 1
    [ -z "$region" ] && printf "Missing required argument --aws-region\n" && return 1

    cd $terraform_dir

    local vars=""

    [[ -n "$cluster_name" ]] && vars="${vars} -var cluster_name=${cluster_name}"
    [[ -n "$region" ]] && vars="${vars} -var region=${region}"
    [[ -n "$prefix_list_id" ]] && vars="${vars} -var prefix_list_ids=[\"${prefix_list_id}\"]"
    [[ -n "$ssh_access_cidr_blocks" ]] && vars="${vars} -var ssh_access_cidr_blocks=[\"${ssh_access_cidr_blocks}\"]"

    echo "==> Syncing remote data directory"
    sync_bai_datadir --aws-region=${region} --mode="pull" || return 1

    echo "==> Initializing terraform"
    _initialize_terraform_backend ${region} || return 1

    echo "==> Calling terraform plan with ${vars}"
    terraform plan --out=$terraform_plan -var data_dir=$data_dir ${vars} || return 1
    terraform apply $terraform_plan || return 1

    terraform output kubectl_config > $kubeconfig || return 1

    echo "==> Creating Kubernetes objects"
    ${kubectl} apply -f $data_dir/fluentd-daemonset.yaml
    ${kubectl} -n kube-system rollout status ds/fluentd

    ${kubectl} apply -f $data_dir/autoscaler-deployment.yaml

    ${kubectl} apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml
    ${kubectl} apply -f $project_dir/metrics-pusher/metrics-pusher-roles.yaml

    echo "==> Adding permission for CodeBuild role to deploy into the Kubernetes cluster"
    _add_codebuild_role || return 1

    echo "==> Adding permission for extra users and roles into the Kubernetes cluster"
    _add_extra_users_roles || return 1

    echo "==> Creating configmap from Terraform outputs"
    _create_configmap_yaml_from_terraform_outputs || return 1

    echo "==> Installing kubeflow operators"
    _install_kubeflow_operators || return 1

    echo "==> Installing Tiller"
    _install_tiller || return 1

    echo "==> Installing kubernetes-dashboard"
    _install_kubernetes_dashboard || return 1

    echo "==> Installing Prometheus"
    _install_prometheus || return 1

    echo "==> Installing kube2iam"
    _install_kube2iam || return 1

    echo "==> Installing zookeeper"
    _install_zookeeper || return 1

    echo "==> Validating infrastructure"
    [[ "$validate" == false ]] || validate_infra $@

    echo "==> Syncing remote data directory"
    sync_bai_datadir --aws-region=${region} --mode="push" || return 1
}

_initialize_terraform_backend() {
    local region=$1
    if [[ ! -f ${data_dir}/backend.tfvars ]] ; then
        local account_id=$(aws sts get-caller-identity | jq -r '.Account') || return 1
        local bucket="bai-terraform-state-${region}-${account_id}"

        if aws s3 ls s3://$bucket; then
            printf "S3 bucket in $data_dir/backend.tfvars already exists. Only creating backend.tfvars file\n"
        else
            printf "Creating bootstrap S3 bucket...\n"
            aws s3 mb s3://${bucket} --region ${region} || return 1
        fi
        cat <<-END > ${data_dir}/backend.tfvars
        bucket="${bucket}"
        key="terraform.tfstate"
        region="${region}"
END
    fi
    # Get bucket name from backend.tfvars
    local s3_bucket_name=$(cat $data_dir/backend.tfvars | grep bucket | awk -F'"' '{print $2}')
    if [[ "$s3_bucket_name" == "" ]] || ! aws s3 ls s3://${s3_bucket_name}; then
        printf "S3 bucket ($s3_bucket_name) in $data_dir/backend.tfvars does not exist!\n"
        return 1
    fi
    echo "-> Calling terraform init"
    terraform init --backend-config=${data_dir}/backend.tfvars || return 1
    echo "-> Calling terraform get"
    terraform get || return 1
}

_install_tiller(){
    ${kubectl} get serviceaccount -n ${tiller_namespace} tiller -o yaml || $kubectl create serviceaccount --namespace ${tiller_namespace} tiller || return 1
    ${kubectl} get clusterrolebinding tiller-cluster-rule -o yaml || $kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=${tiller_namespace}:tiller || return 1

    ${helm} init --wait --service-account tiller || return 1
    #TODO: Secure through TLS?: https://github.com/jonbcampos/kubernetes-series/blob/master/helm/scripts/add_secure_helm.sh
    # TODO: Permissions RBAC
    # https://medium.com/@amimahloof/how-to-setup-helm-and-tiller-with-rbac-and-namespaces-34bf27f7d3c3
}

_install_kubernetes_dashboard(){
    # Remove the old dashboard which might have been installed manually in the cluster
    if [[ "$(${helm} ls -q kubernetes-dashboard)" = "" ]] && [[ $(${kubectl} -n kube-system get --ignore-not-found=true -o json deployment/kubernetes-dashboard) != "" ]]; then
        echo "INFO: Removing old kubernetes dashboard"
        kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/5c5c73639a0a43df552ff839d8e358c2430b71f7/aio/deploy/recommended/kubernetes-dashboard.yaml
    fi
    ${helm} upgrade --install kubernetes-dashboard --namespace ${tiller_namespace} --wait stable/kubernetes-dashboard || return 1
}

_install_prometheus(){
    ${helm} upgrade --install prometheus-operator-1 -f ${k8s_deploy_dir}/prometheus-operator.yaml stable/prometheus-operator || return 1

    # The prometheus node-exporter does not support GPU metrics by default. Thus, adding it separately
    # This also provides per-pod metrics. But this requires that a pod actually allocates a gpu. See
    # https://github.com/MXNetEdge/benchmark-ai/issues/483 for details.
    # https://github.com/NVIDIA/gpu-monitoring-tools/tree/master/exporters/prometheus-dcgm/k8s/pod-gpu-metrics-exporter#pod-gpu-metrics-exporter
    ${kubectl} apply -f ${k8s_deploy_dir}/pod-gpu-metrics-exporter-daemonset.yaml || return 1


    _deploy_grafana_dashboards || return 1

}

_deploy_grafana_dashboards() {
    # Deploy configmaps that contain the grafana dashboards, they will be loaded by the sidecar as long as they
    # have the "grafana_dashboard" label
    for filename in ${k8s_deploy_dir}/grafana-dashboards/*.json; do
        local configmap_name=$(echo "grafana-dashboard-${filename##*/}" | sed 's/.json//g')
        echo "Applying Grafana dashboard '${configmap_name}'"
        ${kubectl} create configmap ${configmap_name} --from-file "${filename}" --dry-run -o yaml | ${kubectl} apply -f - || return 1

        # https://github.com/kubernetes/kubernetes/issues/60295
        ${kubectl} label configmap ${configmap_name} grafana_dashboard=1 --overwrite=true || return 1
    done
}

port_forward() {
    local service_name=""
    for arg in "$@"; do
        case "${arg}" in
        --service=*)
            service_name="${arg#*=}"
            ;;
        esac
    done

    if [ -z "$service_name" ]; then
        printf "Missing required argument --service=[grafana|alertmanager|prometheus]\n" && return 1
    fi

    if [ "$service_name" == "grafana" ]; then
        _port_forward_grafana
    elif [ "$service_name" == "alertmanager" ]; then
        _port_forward_alertmanager
    elif [ "$service_name" == "prometheus" ]; then
        _port_forward_prometheus
    else
        printf "Unknown service $service_name\n" && return 1
    fi
}

_port_forward_grafana(){
    printf "Navigate to 127.0.0.1:3000 to access the Grafana dashboard\nCtrl+C to cancel the session.\n"
    ${kubectl} port-forward deployment/prometheus-operator-1-grafana 3000
}

_port_forward_alertmanager(){
    printf "Navigate to 127.0.0.1:9093 to access the AlertManager dashboard\nCtrl+C to cancel the session.\n"
    ${kubectl} port-forward alertmanager-prometheus-operator-1-alertmanager-0 9093
}

_port_forward_prometheus(){
    printf "Navigate to 127.0.0.1:9090 to access the Prometheus dashboard\nCtrl+C to cancel the session.\n"
    ${kubectl} port-forward prometheus-prometheus-operator-1-prometheus-0 9090
}

_add_codebuild_role(){
    local account_id=$(aws sts get-caller-identity | jq -r '.Account') || return 1

    local bash_src=$(realpath ${BASH_SOURCE})
    local src_dir=$(dirname "${bash_src}")
    (
        export KUBECONFIG=${kubeconfig}
        python3 ${src_dir}/add_permission_to_aws_auth_configmap.py --role-arn "arn:aws:iam::${account_id}:role/code-build-role" --role-username build --role-groups system:masters || return 1
    )
}

_add_extra_users_roles(){
    local bash_src=$(realpath ${BASH_SOURCE})
    local src_dir=$(dirname "${bash_src}")

    for user in $(echo $extra_users | sed "s/,/ /g")
    do
        (
            export KUBECONFIG=${kubeconfig}
            python3 ${src_dir}/add_permission_to_aws_auth_configmap.py --iam-user-arn $user --iam-user-username $(echo $user | sed -n 's/.*\/\(.[a-zA-Z0-9]*\)/\1/p') --iam-user-groups system:masters || return 1
        )
        local exit_code=$?
        [[ "$exit_code" != 0 ]] && echo "[ERROR] Failed with exit code: $exit_code" && return 1
    done

    for role in $(echo $extra_roles | sed "s/,/ /g")
    do
        (
            export KUBECONFIG=${kubeconfig}
            python3 ${src_dir}/add_permission_to_aws_auth_configmap.py --role-arn $ --role-username $(echo $role | sed -n 's/.*\/\(.[a-zA-Z0-9]*\)/\1/p') --role-groups system:masters || return 1
        )
        local exit_code=$?
        [[ "$exit_code" != 0 ]] && echo "[ERROR] Failed with exit code: $exit_code" && return 1
    done
}


__create_kubeflow_namespace(){
    cat << EOF
apiVersion: v1
kind: Namespace
metadata:
  name: kubeflow
EOF
}

_install_kubeflow_operators() {
    (
    _check_kubeflow_app_config

    mkdir $data_dir/kubeflow-ks-app
    cd $data_dir/kubeflow-ks-app

    export KUBECONFIG=$kubeconfig
    ks init ks_app || echo "App already exists, continuing" # If the creation fails, the following cd will fail

    cd ks_app || return 1

    echo "-> Creating kubeflow namespace"
    __create_kubeflow_namespace | ${kubectl} apply -f - || return 1

    echo "-> Creating kubeflow environment"
    ks registry describe kubeflow || ks registry add kubeflow https://github.com/kubeflow/kubeflow/tree/v0.4.1/kubeflow || return 1
    ks env describe "$kubeflow_namespace" || ks env add kubeflow --namespace "$kubeflow_namespace" || return 1

    echo "-> Applying kubeflow stuff"
    ks pkg list --installed -o table | awk '{print $2}' | xargs | grep "mpi-job" || ks pkg install kubeflow/mpi-job || return 1 && \
        ks pkg list --installed -o table | awk '{print $2}' | xargs | grep "mxnet-job" || ks pkg install kubeflow/mxnet-job || return 1 && \
        ks component list | awk '{print $1}' | xargs | grep "mpi-operator" || ks generate mpi-operator mpi-operator || return 1 && \
        ks component list | awk '{print $1}' | xargs | grep "mxnet-operator" || ks generate mxnet-operator mxnet-operator || return 1


    ks apply kubeflow --component mpi-operator && \
        ks apply kubeflow --component mxnet-operator
    )
    local exit_code=$?
    [[ "$exit_code" != 0 ]] && echo "[ERROR] Failed with exit code: $exit_code" && return 1

    return 0
}

_check_kubeflow_app_config() {
    local kubeflow_config_path="${data_dir}/kubeflow-ks-app/ks_app/app.yaml"
    if [[ ! -f ${kubeflow_config_path} ]]; then
        echo "Kubeflow is not initialized yet"
        # Just to be safe
        rm -rf $data_dir/kubeflow-ks-app
        return 0
    fi

    # Check if URL in config is the right one. if not, it's stale and we have to re-create it
    local eks_server_url=$(cat $data_dir/kubeconfig | grep -o -e 'https://.*eks.amazonaws.com')

    if grep -q "$eks_server_url" "${kubeflow_config_path}"
    then
        echo "Kubeflow already has been connected to the correct EKS server"
        # code if found
    else
        echo "Kubeflow is currently not connected to the correct EKS server"
        rm -rf $data_dir/kubeflow-ks-app
    fi
}

_init_service_account() {
    echo "-> Initializing service account for helm"
    ${kubectl} apply -f ${project_dir}/baictl/helm/
}

_init_helm() {
    _init_service_account || return 1
    ${helm} init --upgrade --history-max 200 --wait --service-account tiller || return 1
    echo "-> Updating helm"
    ${helm} repo update || return 1
}

_install_kube2iam() {
    local default_pod_role_name=$(terraform output kube2iam_default_pod_role_name)
    local account_id=$(aws sts get-caller-identity | jq -r '.Account') || return 1
    ${helm} upgrade --install kube2iam \
        --namespace ${tiller_namespace} \
        --set aws.region=${region} \
        --set host.iptables=true \
        --set host.interface=eni+ \
        --set rbac.create=true \
        --set extraArgs.default-role=${default_pod_role_name} \
        --set extraArgs.base-role-arn=arn:aws:iam::${account_id}:role/ \
        --set updateStrategy=RollingUpdate \
        --wait \
        stable/kube2iam || return 1
}

_destroy_helm_charts() {
    echo "-> Destroying helm charts:\n$(helm ls --short)\n"
    ${helm} delete $(${helm} ls --short) --purge || return 1

    _destroy_orphaned_crds || return 1
}

_destroy_orphaned_crds() {
    # https://github.com/helm/charts/issues/9161
    ${kubectl} delete crd prometheuses.monitoring.coreos.com || return 1
    ${kubectl} delete crd prometheusrules.monitoring.coreos.com || return 1
    ${kubectl} delete crd servicemonitors.monitoring.coreos.com || return 1
    ${kubectl} delete crd alertmanagers.monitoring.coreos.com || return 1
}

_install_zookeeper(){
    ${kubectl} apply -f ${k8s_deploy_dir}/zookeeper.yaml || return 1
}

destroy_infra() {
    for arg in "$@"; do
        case "${arg}" in
        --aws-region=*)
            region="${arg#*=}"
            ;;
        esac
    done

    cd ${terraform_dir}

    if [[ -n "$region" ]]; then
        vars="${vars} -var region=${region} -var data_dir=${data_dir}"
    else
        printf "Missing required argument --aws-region\n" && return 1
    fi

    _initialize_terraform_backend ${region} || return 1
    terraform destroy -auto-approve ${vars}


}

get_infra() {
    cd $terraform_dir

    for arg in "$@"; do
        printf "\n----------\n"
        case "${arg}" in
        --nodes)
            $kubectl get nodes -o wide
            ;;
        --aws-es)
            terraform output es_endpoint
            ;;
        --aws-cluster)
            terraform output region
            terraform output eks_cluster_name
            terraform output cluster_endpoint
            ;;
        --aws-bastion-ip)
            terraform output bastion_public_ip
            ;;
        esac
    done
    printf "\n----------\n"
}

__validate_crd() {
    local type=$1
    local namespace=$2

    local kind
    kind=$($kubectl get crd "${type}" --namespace "${namespace}" --output=json | jq .kind --raw-output)
    [[ "$kind" == "CustomResourceDefinition" ]] || return 1
}

__validate_mpi_job() {
    printf "MPI Job is present"
    __validate_crd mpijobs.kubeflow.org "$kubeflow_namespace"
}

__validate_mxnet_job() {
    printf "MXNET Job is present"
    __validate_crd mxjobs.kubeflow.org "$kubeflow_namespace"
}

__validate_zookeeper_pods() {
    printf "ZooKeeper is ready"
    $kubectl wait --for=condition=ready pod --selector=app=zookeeper --timeout=300s > /dev/null || return 1
}

__validate_kubectl() {
    printf "Kubectl is present"
    $kubectl version > /dev/null || return 1
}

__validate_grafana() {
    # TODO: Validate through tunnel method.
    # This method only works if the load balancer is up, which is complicated because it either has to be
    # restricted to Amzn (for example) which then makes it unavailable to ECS (if we don't deploy through our Macs)
    # or we have to use a public endpoint which has security concerns.

    printf "Grafana is ready"
    password=$(
        ${kubectl} get secret --namespace tiller-world grafana -o jsonpath="{.data.admin-password}" | base64 --decode
    )
    host=$(
        ${kubectl} get svc grafana --namespace tiller-world -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
    )

    # TODO: Determine security implications of this call. Generally we should avoid passing credentials as program args
    # according to https://coe.amazon.com/coes/100190
    curl http://admin:${password}@${host}:80/api/health || return 1
}

validate_infra() {
    local all_ok=true
    local rules=(
        __validate_mpi_job \
        __validate_mxnet_job \
        __validate_zookeeper_pods \
        __validate_kubectl
    )

    for rule in "${rules[@]}"; do
        local result=true
        eval "$rule" || result=false

        printf "..."

        [[ "$result" == false ]] && printf "FAILED\n" && all_ok=false || printf "PASSED\n"
    done

    [[ "$all_ok" == true ]] || return 1
}

get_benchmark_logs() {
    local benchmark_name=""

    for arg in "$@"; do
        case "${arg}" in
        --name=*)
            benchmark_name="${arg#*=}"
            ;;
        --aws-es-page-size=*)
            page_size="${arg#*=}"
            ;;
        esac
    done

    [ -z "$benchmark_name" ] && printf "Missing required argument --name\n" && return 1
    [ -z "$page_size" ] && page_size=10000

    _query_fluentd_logs_from_elasticsearch --key "kubernetes.labels.action-id" --value $benchmark_name
}

get_pod_logs() {
    local pod_name=""

    for arg in "$@"; do
        case "${arg}" in
        --name=*)
            pod_name="${arg#*=}"
            ;;
        --aws-es-page-size=*)
            page_size="${arg#*=}"
            ;;
        esac
    done

    [ -z "$pod_name" ] && printf "Missing required argument --name\n" && return 1

    _query_fluentd_logs_from_elasticsearch --key=kubernetes.pod_name --value=${pod_name}
}

_query_fluentd_logs_from_elasticsearch() {
    local value=""

    for arg in "$@"; do
        case "${arg}" in
        --value=*)
            value="${arg#*=}"
            ;;
        --key=*)
            key="${arg#*=}"
            ;;
        --aws-es-page-size=*)
            page_size="${arg#*=}"
            ;;
        esac
    done

    [ -z "$value" ] && printf "Missing required argument --value\n" && return 1
    [ -z "$key" ] && printf "Missing required argument --key\n" && return 1
    [ -z "$page_size" ] && page_size=10000

    cd $terraform_dir

    bastion_pem_filename=$(terraform output bastion_pem_filename)
    bastion_ip=$(terraform output bastion_public_ip)
    es_endpoint=$(terraform output es_endpoint)

    _call_elasticsearch() {
        local url=${1}
        local query_body=${2}
        local curl_cmd="curl -X POST -s -H 'Content-Type: application/json' -d '$query_body' ${es_endpoint}/${url}"
        local result=$(ssh -q -o StrictHostKeyChecking=no -i ${bastion_pem_filename} ubuntu@${bastion_ip} "${curl_cmd}")
        local status=$(echo $result | jq '.status')
        if [[ "$status" -ne "null" ]]; then
            echo "==========================================================================================" >&2
            echo "Error calling elasticsearch on '${url}'. Got status '${status}'" >&2
            echo "Body of query is:" >&2
            echo ${query_body} >&2
            echo "See output for details:" >&2
            echo $result | jq >&2
            echo "==========================================================================================" >&2
            exit 1
        fi
        echo $result
    }

    local current_page_filename=$(mktemp /tmp/bai-current-page.XXXXXX.json)
    # See the "trap" trick: https://stackoverflow.com/questions/687014/removing-created-temp-files-in-unexpected-bash-exit
    trap "{ rm -f $current_page_filename; }" EXIT

    # Use the "scroll" api from Elasticsearch to do pagination
    # see: https://www.elastic.co/guide/en/elasticsearch/reference/6.4/search-request-scroll.html
    local search_query=$(cat <<-END
        {
            "size" : $page_size,
            "query" : {
                "match" : { "${key}":"$value" }
            },
            "sort": [
                {"@timestamp": {"order": "asc"}}
            ]
        }
END
)
    _call_elasticsearch "_search?scroll=1m" "${search_query}" > $current_page_filename

    local scroll_id=$(cat $current_page_filename | jq '._scroll_id')

    while true; do
        local current_page_size=$(cat $current_page_filename | jq '.hits.hits | length')
        if [[ ${current_page_size} == 0 ]];  then
            break
        fi

        # Print the logs
        cat $current_page_filename | jq '.hits.hits[]._source | "[\(.kubernetes.pod_id)-\(.kubernetes.container_name)] \(."@timestamp") \(.log)"' -j

        # Fetch next page
        local next_page_query="{\"scroll\" : \"1m\", \"scroll_id\" : $scroll_id}"
        _call_elasticsearch "_search/scroll" "${next_page_query}" > $current_page_filename
    done
}

run_benchmark() {
    local descriptor=""

    for arg in "$@"; do
        case "${arg}" in
        --descriptor=*)
            descriptor="${arg#*=}"
            ;;
        esac
    done

    [ -z "$descriptor" ] && printf "Missing required argument --descriptor\n" && return 1

    (
        local bash_src=$(realpath ${BASH_SOURCE})
        local src_dir=$(dirname "${bash_src}")

        export PYTHONPATH=${src_dir}/../../../executor/src:${src_dir}/../../../kafka-utils/src

        pushd ${terraform_dir} > /dev/null
        local azs=$(terraform output -json --state=${terraform_state} availability_zones | jq -r '.value | join(" ")')
        popd

        [ -z "${azs}" ] && printf "Availability zones could not be identified from terraform state %s\n" "${terraform_state}" && return 1

        python3 ${src_dir}/../../../executor/src/transpiler/__main__.py --descriptor ${descriptor} --availability-zones ${azs} | ${kubectl} apply -f -
    )
}

delete_benchmark() {
    local benchmark_name=""
    local all=""

    for arg in "$@"; do
        case "${arg}" in
        --all)
            all="1"
            ;;
        --name=*)
            benchmark_name="${arg#*=}"
            ;;
        esac
    done

    [ -n "$all" ] && benchmark_name="--all"
    [ -z "$benchmark_name" ] && printf "Missing required argument --name or --all\n" && return 1

    $kubectl delete job $benchmark_name
}

list_benchmarks() {
    $kubectl get jobs --selector app=benchmark-ai -o wide
}

schedule_benchmark() {
    printf "Not yet implemented\n"
}

show_k8s_dashboard() {
    echo "The Kubernetes dashboard will be available in http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/"
    ${kubectl} proxy
}

sync_bai_datadir() {
    for arg in "$@"; do
        case "${arg}" in
        --aws-region=*)
            region="${arg#*=}"
            ;;
        --mode=*)
            mode="${arg#*=}"
            ;;
        esac
    done

    local account_id=$(aws sts get-caller-identity | jq -r '.Account') || return 1
    local bucket="bai-terraform-state-${region}-${account_id}"

    [ -z "$region" ] && printf "Missing required argument --aws-region\n" && return 1
    [ -z "$mode" ] && printf "Missing required argument --mode\n" && return 1

    if [[ "$mode" == "push" ]]; then
        aws s3 sync $data_dir s3://$bucket --exclude "terraform.tfstate"
    elif [[ "$mode" == "pull" ]]; then
        aws s3 sync s3://$bucket $data_dir --exclude "terraform.tfstate"
    else
        printf "Missing required argument in _sync_bai_datadir, push or pull\n" && return 1
    fi
}

verb=$1
object=$2

shift
shift

verbose=""

terraform_dir=$(dirname $BASH_SOURCE)/cluster
data_dir=${terraform_dir}/.terraform/bai

#Common args
for arg in "$@"; do
    case "${arg}" in
    --data-dir=*)
        data_dir="${arg#*=}"
        ;;
    --verbose)
        verbose="1"
        ;;
    esac
done

mkdir -p ${data_dir}

data_dir=$(realpath ${data_dir})
kubeconfig=$(realpath ${data_dir}/kubeconfig)
kube_config_arg=--kubeconfig=${kubeconfig}
kubectl="kubectl ${kube_config_arg}"
tiller_namespace="kube-system"
helm_config_arg="${kube_config_arg} --tiller-namespace ${tiller_namespace}"
helm="helm ${helm_config_arg}"
terraform_plan=${data_dir}/terraform.plan

terraform_dir=$(realpath ${terraform_dir})
terraform_plan=$(realpath ${terraform_plan})

project_dir=$(realpath $(dirname $BASH_SOURCE)/../../..)
k8s_deploy_dir=$(realpath $(dirname $BASH_SOURCE))/deploy

readonly kubeflow_namespace=kubeflow

case "${object}" in
infra)

    case "${verb}" in
    create)
        create_infra $@
        ;;
    port-forward)
        port_forward $@
        ;;
    destroy)
        destroy_infra $@
        ;;
    get)
        get_infra $@
        ;;
    validate)
        validate_infra $@
        ;;
    sync)
        sync_bai_datadir $@
        ;;
    *)
        print_unsupported_verb $object $verb
        ;;
    esac

    ;;
benchmark)

    case "${verb}" in
    run)
        run_benchmark $@
        ;;
    schedule)
        schedule_benchmark $@
        ;;
    logs)
        get_benchmark_logs $@
        ;;
    delete)
        delete_benchmark $@
        ;;
    list)
        list_benchmarks $@
        ;;
    *)
        print_unsupported_verb $object $verb
        ;;
    esac
    ;;

pod)

    case "${verb}" in
    logs)
        get_pod_logs $@
        ;;
    *)
        print_unsupported_verb $object $verb
        ;;
    esac
    ;;

k8s-dashboard)

    case "${verb}" in
	show)
	    show_k8s_dashboard $@
	    ;;

	*)
        print_unsupported_verb $object $verb
        ;;
    esac
    ;;

*)
    printf "Unknown object"
    ;;
esac
