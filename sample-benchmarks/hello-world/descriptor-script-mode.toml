# BenchmarkAI meta
spec_version = "0.1.0"

# These fields don't have any impact on the job to run, they contain
# merely informative data so the benchmark can be categorized when displayed
# in the dashboard.
[info]
description = """ \
    A hello world example of using Benchmark AI with script mode\
    """

# Labels for metrics
[info.labels]
# Labels and values must be 63 characters or less, beginning and ending with an alphanumeric character
# ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.), and alphanumerics between
# task_name is a mandatory label which will be used as a dimension for this job's metrics
task_name = "Hello_world_script-mode"

# 1. Hardware
[hardware]
instance_type = "t3.small"
strategy = "single_node"
# us-east-1 az that has plenty of t3. Adjust for used region.
aws_zone_id = "use1-az5"

# 2. Environment
[env]
# Docker hub <hub-user>/<repo-name>:<tag> 
docker_image = "python:3-alpine"

# 3. Machine learning related settings: 
# dataset, benchmark code and parameters it takes
[ml]
benchmark_code = "$(BAI_SCRIPTS_PATH)/src/entrypoint.sh"

# 4. Output
[output]
# [Opt] Custom metrics descriptions
# List all required metrics descriptions below.
# Make an entry in same format as the one below.
[[output.metrics]]
# Name of the metric that will appear in the dashboards.
name = "epoch"
# Metric unit (required)
units = "index"
# Pattern for log parsing for this metric.
pattern = "Epoch (\\d+)"

[[output.metrics]]
# Name of the metric that will appear in the dashboards.
name = "train_loss"
# Metric unit (required)
units = "ratio"
# Pattern for log parsing for this metric.
pattern = "train loss (\\d+\\.\\d+|\\d+)"

[[output.metrics]]
# Name of the metric that will appear in the dashboards.
name = "accuracy"
# Metric unit (required)
units = "ratio"
# Pattern for log parsing for this metric.
# This is a literal string: use SINGLE QUOTES
pattern = 'accuracy=([-+]?\d*\.\d+|\d+)'
