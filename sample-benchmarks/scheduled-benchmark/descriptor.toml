# BenchmarkAI meta
spec_version = "0.1.0"

# These fields don't have any impact on the job to run, they contain
# merely informative data so the benchmark can be categorized when displayed
# in the dashboard.
[info]
task_name = "Hello world"
description = """ \
    A hello world example of using Benchmark AI\
    """

# Benchmark schedule in Cron format (https://en.wikipedia.org/wiki/Cron)
# E.g. everyday at midnight
scheduling = "0 0 * * *"

# 1. Hardware
[hardware]
instance_type = "t3.small"
strategy = "single_node"

# 2. Environment
[env]
# Docker hub <hub-user>/<repo-name>:<tag> 
docker_image = "benchmarkai/hello-world:latest"

# 3. Machine learning related settings: 
# dataset, benchmark code and parameters it takes
[ml]
benchmark_code = "python3 hello-world.py"

# 4. Output
[output]
# [Opt] Custom metrics descriptions
# List all required metrics descriptions below.
# Make an entry in same format as the one below.
[[output.metrics]]
# Name of the metric that will appear in the dashboards.
name = "elapsed"
# Metric unit (required)
units = "ms"
# Pattern for log parsing for this metric.
# This is a literal string: use SINGLE QUOTES
pattern = 'elapsed: ([-+]?\d*\.\d+|\d+)'
