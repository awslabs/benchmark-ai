# BenchmarkAI meta
spec_version = "0.1.0"

# These fields don't have any impact on the job to run, they contain
# merely informative data so the benchmark can be categorized when displayed
# in the dashboard.
[info]
task_name = "Title"
description = """ \
    Description of the job. Users might want to include details \
    such as whether it's inference or training, particular aspects \
    of their model, etc.\
    """

# Benchmark definition
# 1. Hardware
[hardware]
instance_type = "t3.medium"
strategy = "inference"

# 2. Environment
[env]
# Docker hub <hub-user>/<repo-name>:<tag>
docker_image = "jlcont/benchmarking:270219"
# Args for the docker container
# [Opt] Whether to run the container in privileged mode (default is false)
privileged = false
# [Opt - default is false] Whether more than 64MB shared memory is needed for containers
# (See docker's -shm option)
extended_shm = true

# 3. Machine learning related settings:
# dataset, benchmark code and parameters it takes
[ml]
benchmark_code = "python /home/benchmark/benchmark_server.py"
# [Opt] Arguments to pass to the script in ml.benchmark_code
# The code is called as defined in ml.benchmark_code, plus the args defined here
# INFERENCE_SERVER_HOST and INFERENCE_SERVER_PORT(_X), where X >= 1, environment variables are
# injected into the benchmark container
args = "--host=${INFERENCE_SERVER_HOST} --port=${INFERENCE_SERVER_PORT} --request-timeout=5 "

# [Opt] 4. Dataset
[data]
# Dataset ID
id = "mnist"
# md5 = "rddytftyfrdr75657fftrtrt11"

# [Opt] Data sources
# List all required data sources below.
# Make an entry for each with the same format as the ones below.
[[data.sources]]
# Data download URI.
src = "s3://mlperf-data-stsukrov/imagenet/train-480px"
# Path where the dataset is stored in the container FS
path = "~/data/tf-imagenet/train"

# Second data source
[[data.sources]]
# Data download URI.
src = "s3://mlperf-data-stsukrov/imagenet/validation-480px"
# Path where the dataset is stored in the container FS
path = "~/data/tf-imagenet/validation"

# Server definition
[server]
# Harware on which to run the server
[server.hardware]
instance_type = "p3.8xlarge"
# The server environment definition
[server.env]
# The server image
docker_image = "jlcont/server:270219"
# Args for the docker container
# [Opt] Whether to run the container in privileged mode (default is false)
privileged = false
# [Opt - default is false] Whether more than 64MB shared memory is needed for containers
# (See docker's -shm option)
extended_shm = true
# array ports that are exposed by the server
ports = [8080, 8081]
# Server start command
start_command = "/opt/bin/server"
# [Opt] Arguments to pass to server start command
start_command_args = "--model=mnist"
# [Opt] Server environment variables
[server.env.vars]
VAR1 = "value1"
VAR2 = "value2"

[server.env.readiness_probe]
path = "/ping"
# skipping port to ensure the first
# port defined in [server.env] ports is used
scheme = "https"
initial_delay_seconds = 5
period_seconds = 5
timeout_seconds = 1
success_threshold = 1
failure_threshold = 5

[[server.models]]
src = "model1"
path = "/path/to/model1"

[[server.models]]
src = "model2"
path = "/path/to/model2"
